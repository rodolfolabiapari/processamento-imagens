%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for tomlew at 2011-03-30 09:16:06 -0300


%% Saved with string encoding Unicode (UTF-8)



@proceedings{sibgrapi2013,
	Address = {Arequipa, Per{\'u}},
	Booktitle = {Sibgrapi},
	Date-Modified = {2013-02-02 19:35:00 -0300},
	Month = {august},
	Publisher = {{IEEE}},
	Title = {Sibgrapi 2013, Proceedings of the XXVI Brazilian Symposium on Computer Graphics and Image Processing},
	Year = {2013}}

@TechReport{fddbTech,
	author = {Vidit Jain and Erik Learned-Miller},
	title =  {FDDB: A Benchmark for Face Detection in Unconstrained Settings},
	institution =  {University of Massachusetts, Amherst},
	year = {2010},
	number = {UM-CS-2010-009}
}

@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Haoxiang2015,
abstract = {In real-world face detection, large visual variations, such as those due to pose, expression, and lighting, demand an advanced discriminative model to accurately differentiate faces from the backgrounds. Consequently, effective models for the problem tend to be computationally prohibitive. To address these two conflicting challenges, we propose a cascade architecture built on convolutional neural networks (CNNs) with very powerful discriminative capability, while maintaining high performance. The proposed CNN cascade operates at multiple resolutions, quickly rejects the background regions in the fast low resolution stages, and carefully evaluates a small number of challenging candidates in the last high resolution stage. To improve localization effectiveness, and reduce the number of candidates at later stages, we introduce a CNN-based calibration stage after each of the detection stages in the cascade. The output of each calibration stage is used to adjust the detection window position for input to the subsequent stage. The proposed method runs at 14 FPS on a single CPU core for VGA-resolution images and 100 FPS using a GPU, and achieves state-of-the-art detection performance on two public face detection benchmarks.},
author = {Haoxiang, Li and Lin},
doi = {10.1109/CVPR.2015.7299170},
file = {:Users/pripyat/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2015 - A convolutional neural network cascade for face detection.pdf:pdf},
isbn = {9781479953134},
issn = {1063-6919},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
pages = {5325--5334},
title = {{A Convolutional Neural Network Approach for Face Identification}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/html/Li{\_}A{\_}Convolutional{\_}Neural{\_}2015{\_}CVPR{\_}paper.html http://www.cs.nyu.edu/{~}xd283/face2012.pdf},
year = {2015}
}
@inproceedings{Giusti2013,
abstract = {Deep Neural Networks now excel at image classification, detection and segmentation. When used to scan images by means of a sliding window, however, their high computational complexity can bring even the most powerful hardware to its knees. We show how dynamic programming can speedup the process by orders of magnitude, even when max-pooling layers are present.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.1700v1},
author = {Giusti, Alessandro and Cireşan, Dan C. and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen},
booktitle = {2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings},
doi = {10.1109/ICIP.2013.6738831},
eprint = {arXiv:1302.1700v1},
file = {:Users/pripyat/Library/Application Support/Mendeley Desktop/Downloaded/Giusti, Ciresan, Masci - 2013 - Fast image scanning with deep max-pooling convolutional neural networks.pdf:pdf},
isbn = {9781479923410},
issn = {1098-6596},
keywords = {Biomedical Imaging,Convolution,Deep Neural Networks,Detection,Forward-Propagation,Max-Pooling,Segmentation,Sliding Window},
pages = {4034--4038},
title = {{Fast image scanning with deep max-pooling convolutional neural networks}},
url = {http://ieeexplore.ieee.org/abstract/document/6738831/},
year = {2013}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L??on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Balya1999,
abstract = {A novel approach to critical parts of face detection problems is given, based on analogic cellular neural network (CNN) algorithms. The proposed CNN algorithms find and help to normalize human faces effectively while their time requirement is a fraction of the previously used methods. The algorithm starts with the detection of heads on color pictures using deviations in color and structure of the human face and that of the background. By normalizing the distance and position of the reference points, all faces should be transformed into the same size and position. For normalization, eyes serve as points of reference. Other CNN algorithm finds the eyes on any grayscale image by searching characteristic features of the eyes and eye sockets. Tests made on a standard database show that the algorithm works very fast and it is reliable.},
author = {Balya, D. and Roska, T.},
doi = {10.1023/A:1008121908145},
issn = {0922-5773},
journal = {Journal of VLSI signal processing systems for signal, image and video technology},
number = {2},
pages = {497--511},
title = {{Face and Eye Detection by CNN Algorithms}},
url = {http://dx.doi.org/10.1023/A:1008121908145},
volume = {23},
year = {1999}
}

@article{berg2004s,
  title={Who’s in the picture},
  author={Berg, Tamara L and Berg, Alexander C and Edwards, Jaety and Forsyth, David A},
  journal={Advances in neural information processing systems},
  volume={17},
  pages={137--144},
  year={2004}
}


%Aigaion2 BibTeX export from LISA - Publications
%Tuesday 14 February 2017 12:58:24 PM

@ARTICLE{Bengio-2009,
    author = {Bengio, Yoshua},
     title = {Learning deep architectures for {AI}},
   journal = {Foundations and Trends in Machine Learning},
    volume = {2},
    number = {1},
      year = {2009},
     pages = {1--127},
      note = {Also published as a book. Now Publishers, 2009.},
       doi = {10.1561/2200000006},
  abstract = {Theoretical results suggest that in order to learn the kind of
complicated functions that can represent high-level abstractions (e.g. in
vision, language, and other AI-level tasks), one may need {\insist deep
architectures}. Deep architectures are composed of multiple levels of non-linear
operations, such as in neural nets with many hidden layers or in complicated
propositional formulae re-using many sub-formulae. Searching the
parameter space of deep architectures is a difficult task, but
learning algorithms such as those for Deep Belief Networks have recently been proposed
to tackle this problem with notable success, beating the state-of-the-art
in certain areas. This paper discusses the motivations and principles regarding
learning algorithms for deep architectures,  in particular those exploiting as
building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines,
used to construct deeper models such as Deep Belief Networks.}
}

@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:Users/pripyat/Library/Application Support/Mendeley Desktop/Downloaded/Glorot, Bengio - Unknown - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf?hc{\_}location=ufi http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
